---
title: "Raisins"
author: "Casual Deep Learning"
format: revealjs
editor: visual
---

# Description

Raisins can be of two species: *Kecimen* and *Besni*.\
We built different models to predict the species on the basis of the dimensions of the raisin example approximated as an ellipse.

## Variables

The predictors that can be used are:

-   Area MajorAxisLength
-   MinorAxisLength
-   Eccentricity
-   ConvexArea
-   Extent
-   Perimeter
-   Class_literal

# Descriptive Analysis

#### Loading Libraries

```{r}
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidymodels)
library(glmnet)
library(estimatr)
library(stats)
library(maxLik)
library(Matrix)
library(caret)
install.packages("performance")
library(performance)
install.packages("see")
library(see)
install.packages("corrplot")
library(corrplot)
install.packages("GGally")
library(GGally)
library(car)
```

#### Loading data-set 

(Aim: visualize dataset, summary, remove Class_literal)

```{r}
data = read.csv("/Users/sarapaolini/Desktop/Statistical_learning/Project/Raisin_Dataset.csv",header=TRUE,sep=";")

data

table(data$Class_literal)
summary(data)

# remove the Class_literal  column  
raisins_corr <- data
raisins <- data[,-8] 
# Class: 1 if raisin is of Kecimen type, 0 if it is Besni
```

### Correlations plots

(Aim: visually represent relationships between variables)

```{r}
cor_table<-cor(raisins) 

corrplot(cor_table, type = "upper",     #first corr plot
         tl.col = "black", tl.srt = 45)

ggcorr(raisins, method = c("everything", "pearson")) #heatmap plot

ggpairs(raisins_corr, columns = 1:6, ggplot2::aes(colour= Class_literal)) #cor by groups
```

### Boxplot

(Aim: Identify outliers)

```{r}
boxplot(raisins$Area)
boxplot(raisins$MajorAxisLength)
boxplot(raisins$MinorAxisLength)
boxplot(raisins$Eccentricity)
boxplot(raisins$ConvexArea)
boxplot(raisins$Extent)
boxplot(raisins$Perimeter)
boxplot(raisins$Class)

```

### Histograms highlighting class differences

(Aim:See more clearly if variables' distribution depend on Class)

```{r}
install.packages("ggthemes")
library(ggthemes)
```

```{r}
theme_set(theme_economist())

ggplot() +
geom_histogram(raisins = subset(x=data, subset=Class==1), 
               aes(x = Area), fill = 'black', alpha = 0.5) +
geom_histogram(raisins = subset(x=data, subset=Class==0), 
                 aes(x = Area), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Area's distribution" )) 


ggplot() + 
geom_histogram(raisins = subset(x=data, subset=Class==1), 
               aes(x = MinorAxisLength), fill = 'black', alpha = 0.5) + 
geom_histogram(raisins = subset(x=data, subset=Class==0), 
               aes(x = MinorAxisLength), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between MinorAxisLength's distribution" )) 


ggplot() + 
geom_histogram(raisins = subset(x=data, subset=Class==1), 
               aes(x = Eccentricity), fill = 'black', alpha = 0.5) + 
geom_histogram(raisins = subset(x=data, subset=Class==0), 
               aes(x = Eccentricity), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Eccentricity's distribution" )) 

ggplot() + 
geom_histogram(raisins = subset(x=data, subset=Class==1), 
               aes(x = ConvexArea), fill = 'black', alpha = 0.5) + 
geom_histogram(raisins = subset(x=data, subset=Class==0), 
               aes(x = ConvexArea), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between ConvexArea's distribution" )) 


ggplot() + 
geom_histogram(raisins = subset(x=data, subset=Class==1), 
               aes(x = Extent), fill = 'black', alpha = 0.5) + 
geom_histogram(raisins = subset(x=data, subset=Class==0), 
               aes(x = Extent), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Extent's distribution" ))

ggplot() + 
geom_histogram(raisins = subset(x=data, subset=Class==1), 
               aes(x = Perimeter), fill = 'black', alpha = 0.5) + 
geom_histogram(raisins = subset(x=data, subset=Class==0), 
               aes(x = Perimeter), fill='yellow', alpha = 0.5) +
ggtitle(paste0("Comparison between Perimeter's distribution" ))


```

# Supervised models

#### Splitting train and test test

```{r}
install.packages("caret")
library(caret)

set.seed(42)
training_index = createDataPartition(raisins$Class, p=0.7, list = FALSE) # index of the train set examples
train = raisins[training_index,]
test = raisins[-training_index,]

```

#### MSE Function

(Aim: to evaluate the performance of predictive models)

```{r}
mse = function(predictions,data,y){
  residuals = (predictions - (data[c(y)]))
  mse = (1/nrow(data))*sum((residuals^2))
  return(mse)
}
```

## MODELS

### 1. OLS

```{r}
# Fit the linear regression model
ols = lm("Class ~ .",data=train)
# Summary of the model
summary(ols)
# Predictions on the test data
ols_test_predictions = predict.lm(ols,newdata = test)
# Histogram of the fitted values
hist(fitted(ols))
# Calculate MSE for the training data
mse_train<-mse(fitted(ols), train, "Class") #training error
mse_train
# Calculate MSE for the test data
mse_test<-mse(ols_test_predictions,test,"Class") #test error
mse_test
```

###  2. ROBUST OLS

```{r}
library(sandwich)
library(lmtest)
library(MASS)

# Fit the robust linear regression model
ols_robust = rlm(Class ~ ., data = train, se_type = "HC2")
# Summary of the robust model
summary(ols_robust)
# Predictions on the test data
ols_robust_test_predictions = predict(ols_robust, newdata = test)
# Histogram of the fitted values
hist(fitted(ols_robust))
# Calculate MSE for the training data
mse(fitted(ols_robust), train, "Class") #training error
# Calculate MSE for the test data
mse(ols_robust_test_predictions, test, "Class") #test error

```

### 3. LOGISTIC

```{r}
install.packages("broom")
library(broom)

# Fit the logistic regression model
logistic = glm(Class ~ ., data = train, family = binomial(link = 'logit'))
# Extract tidy coefficients
tidy_output <- tidy(logistic)
tidy_output

# Histogram of the fitted values
hist(fitted(logistic))
# Predictions on the test data
logistic_test_predictions = predict(logistic, newdata = test)
# Calculate MSE for the training data
mse(fitted(logistic), train, "Class")
# Calculate MSE for the test data
mse(logistic_test_predictions, test, "Class")
```

### 4. RIDGE

```{r}
install.packages("glmnet")
library(glmnet)
# Create the model matrix
x = model.matrix(Class~.-1, data = train)
x
# Define the response variable
y=train$Class
# Fit the ridge regression model
fit.ridge=glmnet(x,y,alpha=0)
fit.ridge$beta
# Plot the ridge coefficients
plot(fit.ridge,xvar="lambda", label = TRUE)
# Perform cross-validation for ridge regression
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
# Extract the coefficients from cross-validation
coef(cv.ridge)
# Calculate mean squared error for the training data
mse(fitted(fit.ridge), train, "Class")
```

### 5. LASSO

```{r}
# Fit the lasso regression model
fit.lasso=glmnet(x,y)
# Plot the lasso coefficients
plot(fit.lasso,xvar="lambda",label=TRUE)
# Perform cross-validation for lasso regression
cv.lasso=cv.glmnet(x,y)
# Extract the coefficients from cross-validation
plot(cv.lasso)
coef(cv.lasso)
# Calculate mean squared error for the training data
mse(fit.lasso, raisins, "Class")
#Predict using the lasso regression model
predict(fit.lasso,newx = x)

```

### 6. TREE

```{r}
install.packages("tree")
library(tree)
# Fit the decision tree model
tree = tree(Class ~ ., data = raisins)
# Plot the decision tree
plot(tree)
text(tree)

tree_test_predictions = predict(tree, newdata = test, type = "tree")

# Make predictions on the training data
tree_predictions = predict(tree, newdata = train[,-8], type = "tree")

# Calculate mean squared error for the test data
mse(tree_test_predictions, test, "Class")
mse(tree_test_predictions, test, "Class")

#plot carino
#valutarlo : prendere predizioni e calcolare mse su training e test
#confusion matrix
```

# Unsupervised models

### 1.PCA

```{r}
df = data[-c(8, 9)]
```

```{r}
# Install and load required packages
install.packages("FactoMineR")
install.packages("factoextra")
install.packages("ggplot2")

library(FactoMineR)
library(factoextra)
library(ggplot2)
```

```{r}
options(scipen = 10)
round((apply(df, 2, mean)), digits = 5); round((apply(df, 2, var)), digits = 5)
```

```{r}
summary(df)
```

```{r}
#correlazione di ogni variabile con ogni componente
pr.out = prcomp(df, scale = TRUE)
pr.out
```

```{r}
summary(pr.out)
```

```{r}
install.packages("factoextra")
library(factoextra)
```

```{r}
fviz_eig(pr.out, addlabels = TRUE, ylim = c(0, 70), main = "Scree Plot of PCA")
fviz_pca_var(pr.out, col.var = "blue", col.quanti.sup = "red", 
             addlabels = TRUE, repel = TRUE)
```

```{r}
#Correct dimensions, show clearly the points but it is not readable

#plot(pr.out$x[, 1], pr.out$x[, 2], type = "n", xlab = "PC1", ylab = "PC2")  
#points(pr.out$x[, 1], pr.out$x[, 2], col = rgb(1, 0, 0, alpha = 0.5), pch = 16) 
#arrows(0, 0, pr.out$rotation[, 1], pr.out$rotation[, 2], length = 0.1, angle = 30)
```

```{r}
#Shows both the dimension and the arrows' label, but not the points
biplot(pr.out)
```

```{r}
#Compromise, arrows length increased

plot(pr.out$x[, 1], pr.out$x[, 2], type = "n", xlab = "PC1", ylab = "PC2") 
points(pr.out$x[, 1], pr.out$x[, 2], col = rgb(1, 0, 0, alpha = 0.5), pch = 16)  
arrows(0, 0, pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, length = 0.1, angle = 30)
text(pr.out$rotation[, 1]*7, pr.out$rotation[, 2]*7, labels = rownames(pr.out[[2]]), pos = 3)
```

```{r}
pcadf <- predict(pr.out, newdata = df)

head(pcadf)
```

### 2.Clustering su tutte le variabili

```{r}
km.out = kmeans(df, 2)

km.out
```

```{r}
#Since clusters do not correspond to a specific category, we cannot estimate accuracy. 
#However, distribution should be 450-450, but it is 189-711, so the algorithm is clearly not adequate for this dataset.
plot (df, col = adjustcolor(km.out$cluster + 1, alpha.f = 0.1),
main = "K- Means Clustering Results with K = 2", pch = 20)
```

### 3. Clustering su PCA

```{r}

#Perform 2-means clustering on the components' values for each observation
km.out2 = kmeans(pcadf, 2)
km.out2
```

```{r}
#Graphical representation of the clusters. 
#Results are slightly better on PC than on initial features
plot (pcadf, col = adjustcolor(km.out2$cluster + 1, alpha.f = 0.5),
main = "K- Means Clustering Results with K = 2", pch = 20)
```
